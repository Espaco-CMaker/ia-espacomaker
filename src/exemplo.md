# ExecuÃ§Ã£o Local de LLaMA 2 7B Quantizado com Streamlit

Bem-vindo ao guia prÃ¡tico para executar o modelo **LLaMA 2 7B quantizado (GGUF 4-bit)** localmente com interface em **Streamlit**! 
Ideal para testes, demonstraÃ§Ãµes e projetos offline com performance leve e responsiva.

## ğŸ“¦ Requisitos Iniciais
- Modelo `.gguf` quantizado (ex: `llama-2-7b-chat.gguf.q4_K_M.bin`)
- Placa de vÃ­deo com **8GB de VRAM** ou superior
- InstalaÃ§Ã£o das bibliotecas:

```bash
pip install streamlit llama-cpp-python
```

## ğŸ› ï¸ Estrutura do Projeto
- `app.py`: Interface Streamlit com campo de entrada e resposta
- `model/`: Pasta onde deve estar o arquivo `.gguf` do modelo

## ğŸ§ª CÃ³digo de Exemplo (`app.py`)
```python
import streamlit as st
from llama_cpp import Llama

MODEL_PATH = "model/llama-2-7b-chat.gguf.q4_K_M.bin"

@st.cache_resource
def load_model():
    return Llama(
        model_path=MODEL_PATH,
        n_ctx=2048,
        n_threads=8,
        n_gpu_layers=20,
        f16_kv=True
    )

llm = load_model()

st.title("ğŸ’¬ Chat Local - LLaMA 2 7B Quantizado")

user_input = st.text_input("Digite sua pergunta:")

if user_input:
    with st.spinner("Pensando..."):
        output = llm(
            f"[INST] {user_input} [/INST]",
            temperature=0.7,
            top_p=0.9,
            max_tokens=512,
            stop=["</s>"]
        )
        resposta = output['choices'][0]['text']
        st.success(resposta.strip())
```

## ğŸ“‹ O que estÃ¡ acontecendo
- O modelo Ã© carregado com cache para desempenho.
- As mensagens do usuÃ¡rio sÃ£o formatadas no estilo `[INST]...[/INST]`.
- A resposta Ã© exibida com `st.success()` apÃ³s o processamento.

## âš¡ Dicas de OtimizaÃ§Ã£o
- Use `n_gpu_layers=20` para aproveitar a GPU (em placas com 8GB).
- Experimente `low_vram=True` para cargas mais leves.
- Para histÃ³rico de conversa, utilize `st.session_state` para armazenar os prompts.

## ğŸš€ Resultados Esperados
- Tempo de resposta entre **1 e 2 segundos** por prompt.
- Totalmente offline e personalizÃ¡vel.
- Ideal para experiÃªncias educacionais, protÃ³tipos ou laboratÃ³rios locais.

---

Caso deseje uma versÃ£o com **histÃ³rico de conversa**, **ajustes de temperatura**, ou **botÃ£o de limpar conversa**, consulte o README completo ou fale comigo! ğŸš€

# ExecuÃ§Ã£o Local de LLaMA 2 7B Quantizado com Streamlit

Bem-vindo ao guia prÃ¡tico para executar o modelo **LLaMA 2 7B quantizado (GGUF 4-bit)** localmente com interface em **Streamlit**! 
Ideal para testes, demonstraÃ§Ãµes e projetos offline com performance leve e responsiva.

## ðŸ“¦ Requisitos Iniciais
- Modelo `.gguf` quantizado (ex: `llama-2-7b-chat.gguf.q4_K_M.bin`)
- Placa de vÃ­deo com **8GB de VRAM** ou superior
- Lembre de instalar um compilador C/C++ recomenda-se a
 utilizaÃ§Ã£o do Build Tools do Visual Studio
- Ã© necessÃ¡rio caso queira utilizar a gpu instalar o ToolKit CUDA da NVIDIA 
- Ã© necessario instalar o CMAKE TOOLS
- InstalaÃ§Ã£o das bibliotecas:

```bash
git clone --recurse-submodules https://github.com/abetlen/llama-cpp-python.git
cd llama-cpp-python

$env:CMAKE_ARGS="-DGGML_CUDA=ON -DGGML_AVX512=ON -DGGML_CUDA_FA_ALL_QUANTS=ON -DGGML_CUDA_MMV_Y=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=4096 -DGGML_CUDA_USE_GRAPHS=ON"
pip install --verbose .[server] // O processo pode demorar
```

## ðŸ› ï¸ Estrutura do Projeto
- `app.py`: Interface Streamlit com campo de entrada e resposta
- `model/`: Pasta onde deve estar o arquivo `.gguf` do modelo

- Arquivo `.gguf` que deve ser baixado diretamento do repositorio do github ou HuggingFace do modelo de preferÃªncia usado no exemplo --> "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/tree/main"

## ðŸ§ª CÃ³digo de Exemplo (`app.py`)
```python
import streamlit as st
from llama_cpp import Llama

MODEL_PATH = "model/llama-2-7b-chat.gguf.Q4_K_M.gguf"

@st.cache_resource
def load_model():
    return Llama(
        model_path=MODEL_PATH,
        n_ctx=2048,
        n_threads=8,
        n_gpu_layers=20,
        f16_kv=True
    )

llm = load_model()

st.title("ðŸ’¬ Chat Local - LLaMA 2 7B Quantizado")

user_input = st.text_input("Digite sua pergunta:")

if user_input:
    with st.spinner("Pensando..."):
        output = llm(
            f"[INST] {user_input} [/INST]",
            temperature=0.7,
            top_p=0.9,
            max_tokens=512,
            stop=["</s>"]
        )
        resposta = output['choices'][0]['text']
        st.success(resposta.strip())
```

## ðŸ“‹ O que estÃ¡ acontecendo
- O modelo Ã© carregado com cache para desempenho.
- As mensagens do usuÃ¡rio sÃ£o formatadas no estilo `[INST]...[/INST]`.
- A resposta Ã© exibida com `st.success()` apÃ³s o processamento.

## âš¡ Dicas de OtimizaÃ§Ã£o
- Use `n_gpu_layers=20` para aproveitar a GPU (em placas com 8GB).
- Experimente `low_vram=True` para cargas mais leves.
- Para histÃ³rico de conversa, utilize `st.session_state` para armazenar os prompts.

## ðŸš€ Resultados Esperados
- Tempo de resposta entre **1 e 2 segundos** por prompt.
- Totalmente offline e personalizÃ¡vel.
- Ideal para experiÃªncias educacionais, protÃ³tipos ou laboratÃ³rios locais.

---

Caso deseje uma versÃ£o com **histÃ³rico de conversa**, **ajustes de temperatura**, ou **botÃ£o de limpar conversa**, consulte o README completo ou fale comigo! ðŸš€
